
from search import search_prompt, search_prompt_hibrido
from llm_handler import LLMHandler

"""
DIFEREN√áAS ENTRE AS FUN√á√ïES DE BUSCA:

1. search_prompt() - BUSCA SIMPLES:
   - Usa apenas busca vetorial (similarity search)
   - Recupera k chunks mais similares semanticamente √† pergunta
   - Aplica pr√©-processamento espec√≠fico apenas para Gemini em perguntas comparativas
   - Mais r√°pida, mas pode perder informa√ß√µes relevantes se os chunks n√£o forem recuperados

2. search_prompt_hibrido() - BUSCA H√çBRIDA:
   - FASE 1: Busca vetorial (como a fun√ß√£o simples)
   - FASE 2: Busca lexical por termos-chave extra√≠dos da pergunta
   - FASE 3: Combina ambos os resultados, removendo duplicatas
   - Extrai termos importantes da pergunta (empresas, valores, palavras-chave)
   - Faz consultas SQL LIKE no banco para encontrar documentos com esses termos
   - Mais completa, captura dados que a busca vetorial pode perder
   - Melhor para consultas comparativas complexas como "maior faturamento"

QUANDO USAR CADA UMA:
- search_prompt(): Para perguntas simples e espec√≠ficas
- search_prompt_hibrido(): Para perguntas comparativas, listas, rankings
  
VANTAGEM DA H√çBRIDA:
Combina a precis√£o sem√¢ntica (vetorial) com a precis√£o lexical (termos exatos),
garantindo que empresas com valores altos sejam encontradas mesmo se n√£o 
estiverem nos top-k chunks da busca vetorial.
"""

def main():
    print("ü§ñ Sistema de Chat - Desafio MBA IA - RAG com Documentos PDF")
    print("Digite suas perguntas ou 'sair' para encerrar")
    print("=" * 50)
    
    # Inicializar LLMHandler uma vez
    print("üîß Inicializando sistema...")
    llm_handler = LLMHandler()
    
    if not llm_handler.is_available():
        print("‚ùå N√£o foi poss√≠vel iniciar o chat. Nenhum modelo LLM dispon√≠vel.")
        print("Certifique-se de que:")
        print("- Pelo menos uma API key est√° configurada (OpenAI ou Google)")
        print("- As vari√°veis de ambiente est√£o corretas no .env")
        return
   
    chain = search_prompt_hibrido(llm_handler=llm_handler)
  
    if not chain:
        print("‚ùå N√£o foi poss√≠vel iniciar o sistema de busca.")
        print("Certifique-se de que:")
        print("- O banco PostgreSQL est√° rodando")
        print("- A ingest√£o foi executada com sucesso")
        print("- As vari√°veis de ambiente est√£o configuradas")
        return
    
    print("‚úÖ Sistema pronto! Fa√ßa suas perguntas sobre o PDF.")
    print(f"ü§ñ Modelo ativo: {llm_handler.get_model_display_name()}")
    print("\nüí° COMANDOS ESPECIAIS:")
    print("  'modelo' - Ver modelo atual")
    print("  'modelos' - Listar todos os modelos")
    print("  'trocar' - Trocar modelo interativamente")
    print("  'status' - Ver status completo")
    print("=" * 50)
    
    while True:
        try:
            # Solicitar pergunta  
            pergunta = input("\nFa√ßa sua pergunta: ").strip()
            
            # Comandos para sair
            if pergunta.lower() in ['sair', 'exit', 'quit', 'bye']:
                print("\nüëã Chat encerrado!")
                break
                
            # Comandos especiais
            if pergunta.lower() == 'modelo':
                print(f"ü§ñ Modelo atual: {llm_handler.get_model_display_name()}")
                continue
                
            if pergunta.lower() in ['modelos', 'lista']:
                llm_handler.list_models()
                continue
                
            if pergunta.lower() in ['trocar', 'switch', 'mudar']:
                llm_handler.select_model_interactive()
                continue
                
            if pergunta.lower() == 'status':
                info = llm_handler.get_model_info()
                print(f"ü§ñ Modelo atual: {info['current_display']}")
                print(f"üìä Modelos dispon√≠veis: {', '.join(info['available_display'])}")
                print(f"üìà Total de modelos: {info['total']}")
                continue
                
            # Ignorar entradas vazias
            if not pergunta:
                continue
                
            # Exibir pergunta  
            print(f"\nPERGUNTA: {pergunta}")
            print(f"ü§ñ Usando: {llm_handler.get_model_display_name()}")
            
            # Processar pergunta e obter resposta
            resposta = search_prompt_hibrido(pergunta, llm_handler=llm_handler)

            if resposta:
                # Exibir resposta  
                print(f"RESPOSTA: {resposta}")
            else:
                print("RESPOSTA: Erro ao processar sua pergunta.")
            
            # Separador  
            print("\n---")
            
        except KeyboardInterrupt:
            print("\n\nüëã Chat encerrado!")
            break
        except Exception as e:
            print(f"‚ùå Erro: {e}")
            print("Tente novamente ou digite 'sair' para encerrar.")

if __name__ == "__main__":
    main()