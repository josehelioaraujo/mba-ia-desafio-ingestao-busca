from search import search_prompt_hibrido
from llm_handler import LLMHandler

def main():
    print("ü§ñ Sistema de Chat - Desafio MBA IA - RAG com Documentos PDF")
    print("Digite suas perguntas ou 'sair' para encerrar")
    print("=" * 50)
    
    # Inicializar LLMHandler
    print("üîß Inicializando sistema...")
    llm_handler = LLMHandler()
    
    if not llm_handler.is_available():
        print("‚ùå N√£o foi poss√≠vel iniciar o chat. Nenhum modelo LLM dispon√≠vel.")
        print("Certifique-se de que:")
        print("- Pelo menos uma API key est√° configurada (OpenAI ou Google)")
        print("- As vari√°veis de ambiente est√£o corretas no .env")
        return
   
    # Testar inicializa√ß√£o do sistema de busca
    chain = search_prompt_hibrido(llm_handler=llm_handler)
  
    if not chain:
        print("‚ùå N√£o foi poss√≠vel iniciar o sistema de busca.")
        print("Certifique-se de que:")
        print("- O banco PostgreSQL est√° rodando")
        print("- A ingest√£o foi executada com sucesso")
        print("- As vari√°veis de ambiente est√£o configuradas")
        return
    
    print("‚úÖ Sistema pronto! Fa√ßa suas perguntas sobre o PDF.")
    print(f"ü§ñ Modelo ativo: {llm_handler.get_model_display_name()}")
    print("\nüí° COMANDOS ESPECIAIS:")
    print("  'modelo' - Ver modelo atual")
    print("  'modelos' - Listar todos os modelos")
    print("  'trocar' - Trocar modelo interativamente")
    print("  'status' - Ver status completo")
    print("=" * 50)
    
    while True:
        try:
            # Solicitar pergunta  
            pergunta = input("\nFa√ßa sua pergunta: ").strip()
            
            # Comandos para sair
            if pergunta.lower() in ['sair', 'exit', 'quit', 'bye']:
                print("\nüëã Chat encerrado!")
                break
                
            # Comandos especiais
            if pergunta.lower() == 'modelo':
                print(f"ü§ñ Modelo atual: {llm_handler.get_model_display_name()}")
                continue
                
            if pergunta.lower() in ['modelos', 'lista']:
                llm_handler.list_models()
                continue
                
            if pergunta.lower() in ['trocar', 'switch', 'mudar']:
                llm_handler.select_model_interactive()
                continue
                
            if pergunta.lower() == 'status':
                info = llm_handler.get_model_info()
                print(f"ü§ñ Modelo atual: {info['current_display']}")
                print(f"üìä Modelos dispon√≠veis: {', '.join(info['available_display'])}")
                print(f"üìà Total de modelos: {info['total']}")
                continue
                
            # Ignorar entradas vazias
            if not pergunta:
                continue
                
            # Processar pergunta
            print(f"\nPERGUNTA: {pergunta}")
            print(f"ü§ñ Usando: {llm_handler.get_model_display_name()}")
            
            resposta = search_prompt_hibrido(pergunta, llm_handler=llm_handler)

            if resposta:
                print(f"RESPOSTA: {resposta}")
            else:
                print("RESPOSTA: Erro ao processar sua pergunta.")
            
            print("\n---")
            
        except KeyboardInterrupt:
            print("\n\nüëã Chat encerrado!")
            break
        except Exception as e:
            print(f"‚ùå Erro: {e}")
            print("Tente novamente ou digite 'sair' para encerrar.")

if __name__ == "__main__":
    main()